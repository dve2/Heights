{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "uVV8-qTSZm4o"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krVlltvmZeY2"
      },
      "outputs": [],
      "source": [
        "!wget https://ml.gan4x4.ru/msu/students/dubrovin/dataset.zip\n",
        "!unzip dataset.zip\n",
        "!pip install segmentation-models-pytorch lightning torchmetrics\n",
        "!wget https://github.com/dve2/Heights/blob/main/weights/epoch%3D931-step%3D11184.ckpt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import nan\n",
        "from torch.utils.data import Dataset\n",
        "from glob import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import warnings\n",
        "import ast\n",
        "#from torchvision import tv_tensors\n",
        "import pickle\n",
        "import torchvision.transforms.functional as F\n",
        "from statistics import mode\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import ToTensor\n",
        "'''\n",
        "def load_cache(filename = \"cachedm.pickle\"):\n",
        "      if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "           cache =  pickle.load(f)\n",
        "        print(f\"Loaded cache from {filename}\")\n",
        "        return cache\n",
        "      return {}\n",
        "'''\n",
        "\n",
        "class CustomDataset2chdm(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_transform = None, exclude = [], cache = None):\n",
        "      images = glob(f\"{root_dir}{os.sep}Images{os.sep}*\")\n",
        "      labels = glob(f\"{root_dir}{os.sep}Masks{os.sep}_*\") # read only continuous heigths\n",
        "      # extract id from paths\n",
        "      im = set(map(lambda x: Path(x).stem,images))\n",
        "      lab = set(map(lambda x: Path(x).stem[1:],labels))\n",
        "      img_without_masks = im - lab\n",
        "      if len(img_without_masks) > 0:\n",
        "        warn_text = f\"Found images without masks {','.join(img_without_masks)}\"\n",
        "        warnings.warn(warn_text)\n",
        "      self.items =  (list((im & lab) - set(exclude)))\n",
        "      self.items.sort()\n",
        "      self.root_dir = root_dir\n",
        "      self.transform = transform\n",
        "      self.target_transform = target_transform\n",
        "      self.max_height = 100\n",
        "      if cache == None:\n",
        "          self.cache = {}\n",
        "      else:\n",
        "        self.cache = cache\n",
        "\n",
        "      #self.load_cache()\n",
        "      #self.scales = self.get_all_scales()\n",
        "\n",
        "    def get_all_scales(self):\n",
        "        scales = []\n",
        "        for name in self.items:\n",
        "            path = self.get_im_path(name)\n",
        "            image, real_w = self.txt2pil(path)\n",
        "            scales.append(self.get_scale(image, real_w))\n",
        "        return np.array(scales)\n",
        "\n",
        "    def get_scale(self, img, real_w):\n",
        "      return  real_w / img.shape[1]   #  micron per pixel; \"[0]\" changed to \"[1]\"\n",
        "\n",
        "    def get_im_path(self,name):\n",
        "      path = f\"{self.root_dir}{os.sep}Images{os.sep}{name}.txt\"\n",
        "      return path\n",
        "\n",
        "    def get_mask_path(self,name):\n",
        "      path = f\"{self.root_dir}{os.sep}Masks{os.sep}_{name}.txt\"\n",
        "      return path\n",
        "\n",
        "    def save_cache(self,filename = \"cache.pickle\"):\n",
        "      with open(filename, 'wb') as f:\n",
        "        pickle.dump(self.cache, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load_cache(self,filename = \"cache.pickle\"):\n",
        "      if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "          self.cache =  pickle.load(f)\n",
        "        print(f\"Loaded cache from {filename}\")\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.items)\n",
        "\n",
        "    def line2tensor(self,line):\n",
        "        txt = line.strip()\n",
        "        parts = txt.split(\"\\t\")\n",
        "        parts = list(filter(len,parts)) #remove empty\n",
        "        if len(parts) <= 2:\n",
        "          return None\n",
        "        numbers = list(map(float, parts))\n",
        "        t = torch.tensor(numbers)\n",
        "        return t\n",
        "\n",
        "\n",
        "    def txt2pil(self, filename):\n",
        "      if filename in self.cache and \"image\" in self.cache[filename]:\n",
        "        return self.cache[filename][\"image\"].copy(), self.cache[filename][\"real_w\"]\n",
        "\n",
        "      # convert list of relative heights to image\n",
        "      with open(filename, encoding='unicode_escape') as file:\n",
        "        x_line = file.readline() # X\n",
        "        x = self.line2tensor(x_line[6:]) # bypass X,nm \"8:\" was replaced by \"5:\"\n",
        "        real_w = (x.max()-x.min()).item() #let it be in microns\n",
        "        units = x_line[3:5]\n",
        "        if units == \"A°\":\n",
        "          real_w = real_w/10000\n",
        "        if units == \"nm\":\n",
        "          real_w = real_w/1000\n",
        "        line = file.readline() # Y, Z skip it\n",
        "        lines = []\n",
        "        for line in file:\n",
        "          if line != '\\n':  #to exclude the last line\n",
        "            pos = line.index('\\t')#position of the first tabulation\n",
        "            line2 = line[(pos + 2):]#exclude Y-coordinate and 2 tabulations after it\n",
        "            t = self.line2tensor(line2)\n",
        "            if t is not None:\n",
        "              lines.append(t)\n",
        "        t = torch.stack(lines)\n",
        "        # Shift to zero\n",
        "        # Because all heights just a difference between current and randomly sampled point\n",
        "        t = t - t.min()\n",
        "        t = t.numpy()\n",
        "        self.cache[filename]= {\"image\": t, \"real_w\" : real_w}\n",
        "      return t, real_w\n",
        "\n",
        "\n",
        "    def load_heights(self, path):\n",
        "      \"\"\"\n",
        "        get heights of some points marked by human\n",
        "      \"\"\"\n",
        "      df = pd.read_excel(path)\n",
        "      return self.fix_format(df)\n",
        "\n",
        "\n",
        "    def get_height_map(self, path):\n",
        "      if not (path in self.cache and \"mask\" in self.cache[path]):\n",
        "          with open(path, 'r') as file:\n",
        "            content = file.read()\n",
        "          x = ast.literal_eval(content)\n",
        "          x = np.array(x)\n",
        "          self.cache[path] = { \"mask\" : x }\n",
        "      return self.cache[path][\"mask\"].copy()\n",
        "      #return x\n",
        "\n",
        "\n",
        "    def __getitem__(self,n):\n",
        "      \"\"\"\n",
        "        img - data(raw heights) from microscope\n",
        "        masks - continious globules height map\n",
        "\n",
        "        real_w - width of the image in microns\n",
        "      \"\"\"\n",
        "      name = self.items[n]\n",
        "      img = self.get_im_path(name)\n",
        "      mask = self.get_mask_path(name)\n",
        "\n",
        "      image, real_w = self.txt2pil(img)\n",
        "      mask = self.get_height_map(mask)\n",
        "\n",
        "      #image, orig_size, scale_factor = self.rescale(image,real_w)\n",
        "      #mask, _, _ = self.rescale(mask,real_w)\n",
        "      scale_factor = 0\n",
        "\n",
        "      if self.transform:\n",
        "        output = self.transform(image=image, mask=mask)\n",
        "        image = output['image']\n",
        "        mask = output['mask'] # here mask is cropped but not normalized\n",
        "        # TODO resize to one scale\n",
        "        if self.target_transform:\n",
        "          mask = self.target_transform(mask)\n",
        "      meta = {\"w\": real_w, 'name' : name, \"scale_factor\": scale_factor} # , centers: [[x1,y1],[x2,y2]]\n",
        "      binary_mask = torch.where(mask != 0, 1, 0)\n",
        "      mask2 = torch.unsqueeze(binary_mask, 0)\n",
        "      im_mask = torch.cat((image, mask2), 0) #creates two channel tensor, where 1 channel is image, 2nd channel is mask\n",
        "      return im_mask, image, mask, meta\n",
        "\n",
        "    def rescale(self,img, real_w):\n",
        "      resize_coeff = 1\n",
        "      h,w = img.shape[:2]\n",
        "      original_size = (h,w)\n",
        "      #most_popular_scale = mode(self.scales)\n",
        "      most_popular_scale = 0.00389862060546875\n",
        "      scale = self.get_scale(img,real_w)\n",
        "      if most_popular_scale != scale:\n",
        "        resize_coeff = most_popular_scale/scale\n",
        "      new_size = tuple((np.array(original_size) / resize_coeff).astype(int).tolist()) # '*' changed to '/'\n",
        "      img = cv2.resize(img, new_size)\n",
        "      return img, original_size, resize_coeff"
      ],
      "metadata": {
        "id": "je33EylelYP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms as T\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "vTTJMMtwlbLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = [8.489298], [9.06547]\n",
        "\n",
        "train_transforms = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean, std),\n",
        "        A.RandomCrop(192, 192),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = A.Compose(\n",
        "    [\n",
        "        A.Normalize(mean, std),\n",
        "        A.CenterCrop(192, 192),\n",
        "        ToTensorV2(),\n",
        "    ]\n",
        ")\n",
        "\n",
        "class NormalizeNonZero(object):\n",
        "    def __init__(self, mean, std):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, x):\n",
        "        mask = x == 0\n",
        "        x -= self.mean\n",
        "        x /= self.std\n",
        "        x[mask] = 0\n",
        "        return x.to(torch.float32)\n",
        "\n",
        "    def denorm(self,x):\n",
        "        mask = x == 0\n",
        "        x *=  self.std\n",
        "        x += self.mean\n",
        "        x[mask] = 0\n",
        "        return x\n",
        "\n",
        "dot_target_mean, dot_target_std = 3.016509424749255, 2.452459479074767\n",
        "nnz = NormalizeNonZero(dot_target_mean, dot_target_std)\n",
        "\n",
        "target_transform = T.Compose([nnz])"
      ],
      "metadata": {
        "id": "jxmy2qxNllBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from math import nan\n",
        "from torch.utils.data import Dataset\n",
        "from glob import glob\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import warnings\n",
        "import ast\n",
        "#from torchvision import tv_tensors\n",
        "import pickle\n",
        "import torchvision.transforms.functional as F\n",
        "from statistics import mode\n",
        "\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torchvision.transforms import ToTensor\n",
        "'''\n",
        "def load_cache(filename = \"cachedm.pickle\"):\n",
        "      if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "           cache =  pickle.load(f)\n",
        "        print(f\"Loaded cache from {filename}\")\n",
        "        return cache\n",
        "      return {}\n",
        "'''\n",
        "\n",
        "class CustomDataset2chdm(Dataset):\n",
        "    def __init__(self, root_dir, transform=None, target_transform = None, exclude = [], cache = None):\n",
        "      images = glob(f\"{root_dir}{os.sep}Images{os.sep}*\")\n",
        "      labels = glob(f\"{root_dir}{os.sep}Masks{os.sep}_*\") # read only continuous heigths\n",
        "      # extract id from paths\n",
        "      im = set(map(lambda x: Path(x).stem,images))\n",
        "      lab = set(map(lambda x: Path(x).stem[1:],labels))\n",
        "      img_without_masks = im - lab\n",
        "      if len(img_without_masks) > 0:\n",
        "        warn_text = f\"Found images without masks {','.join(img_without_masks)}\"\n",
        "        warnings.warn(warn_text)\n",
        "      self.items =  (list((im & lab) - set(exclude)))\n",
        "      self.items.sort()\n",
        "      self.root_dir = root_dir\n",
        "      self.transform = transform\n",
        "      self.target_transform = target_transform\n",
        "      self.max_height = 100\n",
        "      if cache == None:\n",
        "          self.cache = {}\n",
        "      else:\n",
        "        self.cache = cache\n",
        "\n",
        "      #self.load_cache()\n",
        "      #self.scales = self.get_all_scales()\n",
        "\n",
        "    def get_all_scales(self):\n",
        "        scales = []\n",
        "        for name in self.items:\n",
        "            path = self.get_im_path(name)\n",
        "            image, real_w = self.txt2pil(path)\n",
        "            scales.append(self.get_scale(image, real_w))\n",
        "        return np.array(scales)\n",
        "\n",
        "    def get_scale(self, img, real_w):\n",
        "      return  real_w / img.shape[1]   #  micron per pixel; \"[0]\" changed to \"[1]\"\n",
        "\n",
        "    def get_im_path(self,name):\n",
        "      path = f\"{self.root_dir}{os.sep}Images{os.sep}{name}.txt\"\n",
        "      return path\n",
        "\n",
        "    def get_mask_path(self,name):\n",
        "      path = f\"{self.root_dir}{os.sep}Masks{os.sep}_{name}.txt\"\n",
        "      return path\n",
        "\n",
        "    def save_cache(self,filename = \"cache.pickle\"):\n",
        "      with open(filename, 'wb') as f:\n",
        "        pickle.dump(self.cache, f, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    def load_cache(self,filename = \"cache.pickle\"):\n",
        "      if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "          self.cache =  pickle.load(f)\n",
        "        print(f\"Loaded cache from {filename}\")\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.items)\n",
        "\n",
        "    def line2tensor(self,line):\n",
        "        txt = line.strip()\n",
        "        parts = txt.split(\"\\t\")\n",
        "        parts = list(filter(len,parts)) #remove empty\n",
        "        if len(parts) <= 2:\n",
        "          return None\n",
        "        numbers = list(map(float, parts))\n",
        "        t = torch.tensor(numbers)\n",
        "        return t\n",
        "\n",
        "\n",
        "    def txt2pil(self, filename):\n",
        "      if filename in self.cache and \"image\" in self.cache[filename]:\n",
        "        return self.cache[filename][\"image\"].copy(), self.cache[filename][\"real_w\"]\n",
        "\n",
        "      # convert list of relative heights to image\n",
        "      with open(filename, encoding='unicode_escape') as file:\n",
        "        x_line = file.readline() # X\n",
        "        x = self.line2tensor(x_line[6:]) # bypass X,nm \"8:\" was replaced by \"5:\"\n",
        "        real_w = (x.max()-x.min()).item() #let it be in microns\n",
        "        units = x_line[3:5]\n",
        "        if units == \"A°\":\n",
        "          real_w = real_w/10000\n",
        "        if units == \"nm\":\n",
        "          real_w = real_w/1000\n",
        "        line = file.readline() # Y, Z skip it\n",
        "        lines = []\n",
        "        for line in file:\n",
        "          if line != '\\n':  #to exclude the last line\n",
        "            pos = line.index('\\t')#position of the first tabulation\n",
        "            line2 = line[(pos + 2):]#exclude Y-coordinate and 2 tabulations after it\n",
        "            t = self.line2tensor(line2)\n",
        "            if t is not None:\n",
        "              lines.append(t)\n",
        "        t = torch.stack(lines)\n",
        "        # Shift to zero\n",
        "        # Because all heights just a difference between current and randomly sampled point\n",
        "        t = t - t.min()\n",
        "        t = t.numpy()\n",
        "        self.cache[filename]= {\"image\": t, \"real_w\" : real_w}\n",
        "      return t, real_w\n",
        "\n",
        "\n",
        "    def load_heights(self, path):\n",
        "      \"\"\"\n",
        "        get heights of some points marked by human\n",
        "      \"\"\"\n",
        "      df = pd.read_excel(path)\n",
        "      return self.fix_format(df)\n",
        "\n",
        "\n",
        "    def get_height_map(self, path):\n",
        "      if not (path in self.cache and \"mask\" in self.cache[path]):\n",
        "          with open(path, 'r') as file:\n",
        "            content = file.read()\n",
        "          x = ast.literal_eval(content)\n",
        "          x = np.array(x)\n",
        "          self.cache[path] = { \"mask\" : x }\n",
        "      return self.cache[path][\"mask\"].copy()\n",
        "      #return x\n",
        "\n",
        "\n",
        "    def __getitem__(self,n):\n",
        "      \"\"\"\n",
        "        img - data(raw heights) from microscope\n",
        "        masks - continious globules height map\n",
        "\n",
        "        real_w - width of the image in microns\n",
        "      \"\"\"\n",
        "      name = self.items[n]\n",
        "      img = self.get_im_path(name)\n",
        "      mask = self.get_mask_path(name)\n",
        "\n",
        "      image, real_w = self.txt2pil(img)\n",
        "      mask = self.get_height_map(mask)\n",
        "\n",
        "      #image, orig_size, scale_factor = self.rescale(image,real_w)\n",
        "      #mask, _, _ = self.rescale(mask,real_w)\n",
        "      scale_factor = 0\n",
        "\n",
        "      if self.transform:\n",
        "        output = self.transform(image=image, mask=mask)\n",
        "        image = output['image']\n",
        "        mask = output['mask'] # here mask is cropped but not normalized\n",
        "        # TODO resize to one scale\n",
        "        if self.target_transform:\n",
        "          mask = self.target_transform(mask)\n",
        "      meta = {\"w\": real_w, 'name' : name, \"scale_factor\": scale_factor} # , centers: [[x1,y1],[x2,y2]]\n",
        "      binary_mask = torch.where(mask != 0, 1, 0)\n",
        "      mask2 = torch.unsqueeze(binary_mask, 0)\n",
        "      im_mask = torch.cat((image, mask2), 0) #creates two channel tensor, where 1 channel is image, 2nd channel is mask\n",
        "      return im_mask, image, mask, meta\n",
        "\n",
        "    def rescale(self,img, real_w):\n",
        "      resize_coeff = 1\n",
        "      h,w = img.shape[:2]\n",
        "      original_size = (h,w)\n",
        "      #most_popular_scale = mode(self.scales)\n",
        "      most_popular_scale = 0.00389862060546875\n",
        "      scale = self.get_scale(img,real_w)\n",
        "      if most_popular_scale != scale:\n",
        "        resize_coeff = most_popular_scale/scale\n",
        "      new_size = tuple((np.array(original_size) / resize_coeff).astype(int).tolist()) # '*' changed to '/'\n",
        "      img = cv2.resize(img, new_size)\n",
        "      return img, original_size, resize_coeff\n",
        "\n",
        "\n",
        "def load_cache(filename = \"cachedm.pickle\"):\n",
        "      if os.path.isfile(filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "           cache =  pickle.load(f)\n",
        "        print(f\"Loaded cache from {filename}\")\n",
        "        return cache\n",
        "      return None\n",
        "cache = load_cache()"
      ],
      "metadata": {
        "id": "vKxpyqYLlnK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_train = CustomDataset2chdm(\"Splitted Dataset/Train\", transform  = train_transforms, target_transform = target_transform, cache = cache)\n",
        "loader_train = DataLoader(ds_train, batch_size=16, shuffle=True, num_workers=2)\n",
        "#ds_train = CustomDataset(\"Splitted Dataset/Train\")\n",
        "\n",
        "ds_val = CustomDataset2chdm(\"Splitted Dataset/Val\", transform  = val_transforms, target_transform=target_transform, cache = cache)\n",
        "loader_val = DataLoader(ds_val, batch_size=4, shuffle=False, num_workers=2)\n",
        "#ds_val = CustomDataset(\"Splitted Dataset/Val\")\n",
        "\n",
        "ds_test = CustomDataset2chdm(\"Splitted Dataset/Test\", transform  = val_transforms, target_transform=target_transform, cache = cache)\n",
        "loader_test = DataLoader(ds_test, batch_size=4, shuffle=False, num_workers=2)\n",
        "#ds_test = CustomDataset(\"Splitted Dataset/Test\")"
      ],
      "metadata": {
        "id": "Tu6dh7I-plex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchmetrics import MeanSquaredError\n",
        "from typing import Any, Optional, Sequence, Union\n",
        "from torch import Tensor\n",
        "\n",
        "class ZeroAwareMSE(MeanSquaredError):\n",
        "    def update(self, preds: Tensor, target: Tensor) -> None:\n",
        "        target_sum = torch.sum(target).item()\n",
        "        if target_sum == 0:#change preds and targets in such a way that the result would be zero tensor (not works directly)\n",
        "            preds = preds - preds + 1\n",
        "            target = target + 1\n",
        "        mask = target != 0\n",
        "        return super().update(preds[mask],target[mask])\n"
      ],
      "metadata": {
        "id": "67bQWP-Rlq3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def MSELoss_mask(pred, target): #MSE loss calculated only inside the masks (which are contained in the targets)\n",
        "  mse_loss = MSELoss(reduction='sum')  #calculates sum of the squared errors (without division by n)\n",
        "  loss = mse_loss(pred, target) #calculates sum of the squared errors (without division by n)\n",
        "  target_binary = torch.where(target != 0, 1, 0) #writing 1 in each unmasked pixel and 0 in each masked pixel (for calculation the number of unmasked pixels)\n",
        "  n_ummasked_pxls = target_binary.sum()#.item() #calculating the number of unmasked pixels\n",
        "  if n_ummasked_pxls == 0:\n",
        "    return 0\n",
        "\n",
        "  return loss/n_ummasked_pxls\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class CustomLoss(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(CustomLoss, self).__init__()\n",
        "\n",
        "  def forward(self, pred, target):\n",
        "    return MSELoss_mask(pred, target)"
      ],
      "metadata": {
        "id": "ML01J5fFlrWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lightning as L\n",
        "from torchmetrics import MeanSquaredError\n",
        "from torch.nn import MSELoss\n",
        "import torch\n",
        "\n",
        "class Lit(L.LightningModule):\n",
        "    def __init__(self, model, lr=0.0025):\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.criterion = CustomLoss()\n",
        "        self.save_hyperparameters()\n",
        "        self.metric_train = ZeroAwareMSE()\n",
        "        self.metric_train_dn = ZeroAwareMSE()\n",
        "        self.metric_val = ZeroAwareMSE()\n",
        "        self.metric_val_dn = ZeroAwareMSE()\n",
        "        self.metric_test = ZeroAwareMSE()\n",
        "        self.metric_test_dn = ZeroAwareMSE()  #??\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
        "        return optimizer\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        _, imgs, masks, meta = batch\n",
        "        predicted_masks = self.get_prediction(batch)\n",
        "        #print(predicted_masks, masks)\n",
        "        loss = self.criterion(predicted_masks, masks)\n",
        "        #loss_dn = self.criterion(nnz.denorm(predicted_masks), nnz.denorm(masks))\n",
        "        self.log(\"Loss/\", loss.item(), prog_bar=False)\n",
        "        #self.log(\"Loss_dn/\", loss_dn.item(), prog_bar=False)\n",
        "        self.metric_train.update(predicted_masks, masks)\n",
        "        #self.metric_train_dn.update(nnz.denorm(predicted_masks), nnz.denorm(masks))\n",
        "        return loss\n",
        "\n",
        "    def get_prediction(self,batch):\n",
        "        im_masks, imgs, masks, meta = batch\n",
        "        predicted_masks = self.model(im_masks).squeeze(1)\n",
        "        predicted_masks = self.postprocess(predicted_masks, masks)\n",
        "        return predicted_masks\n",
        "\n",
        "    def postprocess(self, pred, mask):\n",
        "        pred = pred.squeeze(1)\n",
        "        pred[mask == 0] = 0\n",
        "        return pred\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        _, imgs, masks, meta = batch\n",
        "        predicted_masks = self.get_prediction(batch)\n",
        "        self.metric_val.update(predicted_masks, masks)\n",
        "        self.metric_val_dn.update(nnz.denorm(predicted_masks), nnz.denorm(masks))\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        self.log(\"MSE/val\", self.metric_val.compute(), prog_bar=True)\n",
        "        self.log(\"MSE/val_dn\", self.metric_val_dn.compute(), prog_bar=True)\n",
        "        self.metric_val.reset()\n",
        "        self.metric_val_dn.reset()\n",
        "\n",
        "    def on_train_epoch_end(self):\n",
        "        self.log(\"MSE/train\", self.metric_train.compute())\n",
        "        #self.log(\"MSE_dn/train\", self.metric_train_dn.compute())\n",
        "        self.metric_train.reset()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        im_masks, imgs, masks, _ = batch\n",
        "        predicted_masks = self.get_prediction(batch)\n",
        "        self.log(\"MSE/test\", self.metric_test.compute(), prog_bar=True)\n",
        "        self.metric_test.update(predicted_masks, masks)\n",
        "        self.metric_test_dn.update(nnz.denorm(predicted_masks), nnz.denorm(masks))#??\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        self.log(\"MSE/test\", self.metric_test.compute(), prog_bar=True)\n",
        "        self.log(\"MSE/test_dn\", self.metric_test_dn.compute(), prog_bar=True)\n",
        "        self.metric_test.reset()\n",
        "        self.metric_test_dn.reset()"
      ],
      "metadata": {
        "id": "w2ivf6U_lwIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import segmentation_models_pytorch as smp\n",
        "\n",
        "smp_unet = smp.Unet(\n",
        "    encoder_name=\"efficientnet-b0\",  # choose encoder\n",
        "    encoder_weights=None,  # use `imagenet` pre-trained weights for encoder initialization\n",
        "    in_channels=2,  # model input channels 3 for RGB\n",
        "    classes=1,  # model output channels (number of classes in mask)\n",
        ")"
      ],
      "metadata": {
        "id": "gQGnHolflzfr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lightning.pytorch.loggers import TensorBoardLogger\n",
        "from lightning.pytorch.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='MSE/val',\n",
        "    #monitor='MSE/train',\n",
        "    dirpath='lightning_logs',\n",
        "    #filename='sample-mnist-{epoch:02d}-{val_loss:.2f}',\n",
        "    save_top_k=5,\n",
        "    mode='min',\n",
        "    #every_n_epochs=10\n",
        ")\n",
        "\n",
        "lit_model = Lit(smp_unet)\n",
        "\n",
        "#To continue from checkpoint:\n",
        "#lit_model = Lit.load_from_checkpoint(\"/home/jupyter/datasphere/project/lightning_logs/epoch=361-step=4344.ckpt\")\n",
        "#model = lit_model.model\n",
        "#model.train()\n",
        "\n",
        "logger = TensorBoardLogger(\"lightning_logs\", name=\"SMPUnet\")\n",
        "trainer = L.Trainer(\n",
        "    max_epochs=5,\n",
        "    logger=logger,\n",
        "    log_every_n_steps=5,\n",
        "    callbacks=[checkpoint_callback]\n",
        ")  # def on_validation_epoch_start(self):"
      ],
      "metadata": {
        "id": "IBoOEfP5pzK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "bare_filename = 'epoch=931-step=11184'\n",
        "model_path = f\"{bare_filename}.ckpt\"\n",
        "lit_model1 = Lit.load_from_checkpoint(model_path)\n",
        "model = lit_model1.model\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "_81mBlxTl1Or"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XrMZD_apl53U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "def get_dotted_cropped_test_mask(flnm):\n",
        "    root_dir = \"Splitted Dataset/Test\"\n",
        "    path = f\"{root_dir}{os.sep}Masks{os.sep}_{flnm}.txt\"\n",
        "    with open(path, 'r') as file:\n",
        "        content = file.read()\n",
        "        x = ast.literal_eval(content)\n",
        "        x = np.array(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "_-JfpgLumOPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dot_mse = []\n",
        "for item in ds_test:\n",
        "    im_mask, img, mask, meta = item\n",
        "    pred = model(im_mask.unsqueeze(0))\n",
        "    d_pred = nnz.denorm(pred)\n",
        "    d_pred = d_pred.squeeze(0).squeeze(0)\n",
        "    d_pred[mask ==0] =0\n",
        "    d_mask = nnz.denorm(mask)\n",
        "    flnm = meta['name']\n",
        "    dotted_mask = get_dotted_cropped_test_mask(flnm)\n",
        "    t = torch.from_numpy(dotted_mask)\n",
        "    center_crop = [A.CenterCrop(192, 192)(image = t)]\n",
        "    ten_cr = center_crop[0]['image']\n",
        "    zmse = ZeroAwareMSE()\n",
        "    dot_mse.append(zmse(d_pred,ten_cr).item())\n",
        "print(\"Mean dotted MSE on test =\", np.mean(dot_mse))"
      ],
      "metadata": {
        "id": "huZp2g3OmQZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "im_mask, img, mask, meta = ds_test[0]\n",
        "pred = model(im_mask.unsqueeze(0))\n",
        "d_pred = nnz.denorm(pred)\n",
        "d_pred = d_pred.squeeze(0).squeeze(0)\n",
        "d_pred[mask ==0] =0\n",
        "d_mask = nnz.denorm(mask)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(20,5))\n",
        "ttt1 = img.squeeze(0).detach().cpu().numpy()\n",
        "plt.subplot(1,4,1)\n",
        "ttt1 = plt.imshow(ttt1)\n",
        "ttt2 = d_mask.squeeze(0).detach().cpu().numpy()\n",
        "plt.subplot(1,4,2)\n",
        "ttt2 = plt.imshow(ttt2)\n",
        "ttt3 = d_pred.squeeze(0).detach().cpu().numpy()\n",
        "plt.subplot(1,4,3)\n",
        "ttt3 = plt.imshow(ttt3)\n"
      ],
      "metadata": {
        "id": "7VNuicEZmbgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}